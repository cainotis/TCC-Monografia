%!TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

%% ------------------------------------------------------------------------- %%

\chapter{\emph{Deep Q-Network}}
\label{cap:dqn}

% \section{O que é \emph{Deep Q-Network}?}
% \label{sec:o-que-e-dqn}

\enlargethispage{.5\baselineskip}
\emph{Deep Q-Network}\index{Deep Q-Network} ou DQN é um algoritmo desenvolvido em 2015 pela DeepMind\index{DeepMind} para aprendizado por reforço ~\citep{Human-level-control}. O DQN é um dos primeiros algoritmos a ter sucesso em adicionar \emph{deep neural network} a métodos de aprendizagem por reforço, visto que, ele é uma variação do \emph{Q-Learning}, um clássico algoritmo de aprendizado por reforço, cujos \emph{Q-Values} são, ao contrário do método clássico que armazena os valores em uma tabela, aproximados por uma \emph{deep network}. As tentativas anteriores de criar um algoritmo que une o \emph{deep learning} e o aprendizado por reforço falharam em virtude dos métodos de \emph{deep learning} a sofrer \emph{overfitting} o que deixa o sistema instável, contudo a equipe de pesquisadores do DeepMind foi capaz de resolver esse problema ao remover a correlação da sequência de observação com a aleatorização dos dados e ao reduzir a correlação com o valor alvo com a atualização deles de forma periódica.

\section{Q-learning}
\label{sec:q-learning}

\enlargethispage{.5\baselineskip}

\emph{Q-learning}\index{Q-learning} é um modelo de inteligência artificial desenvolvido para se espelhar no processo com o qual os animais aprendem ~\citep{Watkins:PhD}. Dessa forma ele aprende direto das interações com o ambiente ao invés de um modelo do ambiente. Como resultado o agente não precisa de um modelo desenvolvido a priori e se reduz o risco do agente introduzir bias que gerem uma grande perda de resultados quando estiver interagindo com o ambiente real. Além disso, o \emph{Q-learning} utiliza uma política diferente para colher os dados da política que ele tenta otimizar, o que resulta em uma maior eficiência amostral em virtude do reuso mais eficiente dos dados ~\citep{Nguyen_La_2019}.


No entanto, o algoritmo do \emph{Q-learning} depende da criação, preenchimento e armazenamento de uma tabela com os valores aproximados para cada estado-ação do ambiente, chamada de \emph{Q-table}\index{Q-table}. O uso de tabelas inviabiliza a utilização do algoritmo sem modificações para problemas mais complexos, por causa do armazenamento necessário, ao passo que em ambientes mais simples o algoritmo possui bons resultados, além de servir de base para modelos mais complexos de aprendizado por reforço. O \emph{Q-learning} na fase de treino possui uma probabilidade de escolher uma ação de forma aleatória para aumentar a exploração do ambiente, o que reduz as chances de ficar preso em ótimos locais, e constantemente utiliza do resultado das ações para atualizar os valores de $Q(S, A)$ conforme a seguinte equação de Bellman \index{Bellman}\ref{eq:Bellman}~\citep{Bellman_1954}, onde $S$ é o estado, $A$ é a ação, $\alpha$ é a taxa de aprendizado, $\gamma$ é o fator de desconto, $R$ é a recompensa e $t$ é o instante de tempo.

\begin{equation}
	\label{eq:Bellman}
	Novo \ Q(S_{t}, A_{t}) = Q(S_{t}, A_{t}) + \alpha * (R_{t} + \gamma * maxQ(S_{t+1}, a) - Q(S_{t}, A_{t}))
\end{equation}
